import cv2
import mediapipe as mp
import pyautogui
import keyboard

mp_hands = mp.solutions.hands.HandLandmark
width, height = pyautogui.size()
hands = mp.solutions.hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)
cap = cv2.VideoCapture(0)
pyautogui.PAUSE = 0

def extended(finger_tip, finger_mcp, wrist):
    # Finger
    x = finger_tip.y - wrist.y
    y = finger_mcp.y - wrist.y
    
    if x < 0:
        if x < y:
            return True
        else:
            return False
    else:
        if x > y:
            return True
        else:
            return False

while cap.isOpened():
    ret, frame = cap.read()
    
    if not ret:
        break
    
    frame = cv2.flip(frame, 1)

    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(image_rgb)

    if results.multi_hand_landmarks:
        # Get landmarks of the first hand (assuming one hand is visible)
        hand_landmarks = results.multi_hand_landmarks[0]

        index_finger_tip = hand_landmarks.landmark[mp_hands.INDEX_FINGER_TIP]
        index_finger_mcp = hand_landmarks.landmark[mp_hands.INDEX_FINGER_MCP]
        middle_finger_tip = hand_landmarks.landmark[mp_hands.MIDDLE_FINGER_TIP]
        middle_finger_mcp = hand_landmarks.landmark[mp_hands.MIDDLE_FINGER_MCP]
        wrist = hand_landmarks.landmark[mp_hands.WRIST]

        if extended(index_finger_tip, index_finger_mcp, wrist):
            if extended(middle_finger_tip, middle_finger_mcp, wrist):
                pyautogui.mouseDown(button="left")
            else:
                pyautogui.mouseUp(button="left")
            cx, cy = int(index_finger_tip.x * width), int(index_finger_tip.y * height)
        
            pyautogui.moveTo(cx, cy, duration=0)
                

    if keyboard.is_pressed('q'):
        break

cap.release()
cv2.destroyAllWindows()
